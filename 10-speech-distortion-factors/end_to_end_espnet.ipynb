{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "asr_lab_04_espnet.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6RcyxmRJGqlY"
      },
      "source": [
        "# Практика №4\n",
        "\n",
        "Теперь мы построим и обучим простую end-to-end модель. Будем работать с пропатченной версией уже готового [пайплайна](https://www.assemblyai.com/blog/end-to-end-speech-recognition-pytorch). Также нам пригодится [ESPnet](https://github.com/espnet/espnet) для использования модели [Transformer](http://jalammar.github.io/illustrated-transformer/) в качестве энкодера."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iDbO_rrWGq7j"
      },
      "source": [
        "### Bootstrap"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WzJyomV1JaLp"
      },
      "source": [
        "!pip install -q sentencepiece torchaudio"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TROAsHTXHWik"
      },
      "source": [
        "!gdown --id '1skrVbNyrhBLeceGS9CV9uIw_gvo1JiA6'\n",
        "\n",
        "!rm -rf lab4\n",
        "!unzip -q lab4.zip\n",
        "!rm -rf lab4.zip sample_data\n",
        "%cd lab4"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m4wcCtkIH2dn"
      },
      "source": [
        "import math\n",
        "import os\n",
        "\n",
        "import numpy as np\n",
        "import sentencepiece as spm\n",
        "import torch\n",
        "import torch.utils.data as data\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "import torchaudio\n",
        "from espnet.nets.pytorch_backend.transformer.embedding import PositionalEncoding\n",
        "from espnet.nets.pytorch_backend.transformer.encoder_layer import EncoderLayer\n",
        "from espnet.nets.pytorch_backend.transformer.repeat import repeat\n",
        "from espnet.nets.pytorch_backend.transformer.attention import MultiHeadedAttention\n",
        "from espnet.nets.pytorch_backend.transformer.positionwise_feed_forward import (\n",
        "    PositionwiseFeedForward,\n",
        ")\n",
        "from espnet.nets.pytorch_backend.transformer.layer_norm import LayerNorm\n",
        "from espnet.nets.pytorch_backend.nets_utils import make_pad_mask\n",
        "\n",
        "from utils import TextTransform\n",
        "from utils import cer\n",
        "from utils import wer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NaESUZiHJgfN"
      },
      "source": [
        "train_audio_transforms = torch.nn.Sequential(\n",
        "    torchaudio.transforms.MelSpectrogram(\n",
        "        sample_rate=16000, n_fft=400, hop_length=160, n_mels=80\n",
        "    ),\n",
        "    torchaudio.transforms.FrequencyMasking(freq_mask_param=30),\n",
        "    torchaudio.transforms.TimeMasking(time_mask_param=100),\n",
        ")\n",
        "\n",
        "valid_audio_transforms = torchaudio.transforms.MelSpectrogram(\n",
        "    sample_rate=16000, n_fft=400, hop_length=160, n_mels=80\n",
        ")\n",
        "\n",
        "text_transform = TextTransform()\n",
        "\n",
        "# -----------------------------TODO №2-----------------------------------\n",
        "# Заменить графемный токенайзер на сабвордовый TextTransformBPE\n",
        "# -----------------------------------------------------------------------\n",
        "\n",
        "\n",
        "def data_processing(data, data_type=\"train\"):\n",
        "    spectrograms = []\n",
        "    labels = []\n",
        "    input_lengths = []\n",
        "    label_lengths = []\n",
        "    for (waveform, _, utterance, _, _, _) in data:\n",
        "        if data_type == \"train\":\n",
        "            spec = train_audio_transforms(waveform).squeeze(0).transpose(0, 1)\n",
        "        elif data_type == \"valid\":\n",
        "            spec = valid_audio_transforms(waveform).squeeze(0).transpose(0, 1)\n",
        "        else:\n",
        "            raise Exception(\"data_type should be train or valid\")\n",
        "        spectrograms.append(spec)\n",
        "        label = torch.Tensor(text_transform.text_to_int(utterance.lower()))\n",
        "        labels.append(label)\n",
        "        input_lengths.append(spec.shape[0])\n",
        "        label_lengths.append(len(label))\n",
        "\n",
        "    spectrograms = (\n",
        "        torch.nn.utils.rnn.pad_sequence(spectrograms, batch_first=True)\n",
        "        .unsqueeze(1)\n",
        "        .transpose(2, 3)\n",
        "    )\n",
        "    labels = torch.nn.utils.rnn.pad_sequence(labels, batch_first=True)\n",
        "\n",
        "    return spectrograms, labels, input_lengths, label_lengths\n",
        "\n",
        "\n",
        "def GreedyDecoder(\n",
        "    output, labels, label_lengths, blank_label=28, collapse_repeated=True\n",
        "):\n",
        "    arg_maxes = torch.argmax(output, dim=2)\n",
        "    decodes = []\n",
        "    targets = []\n",
        "    for i, args in enumerate(arg_maxes):\n",
        "        decode = []\n",
        "        targets.append(\n",
        "            text_transform.int_to_text(labels[i][: label_lengths[i]].tolist())\n",
        "        )\n",
        "        for j, index in enumerate(args):\n",
        "            if index != blank_label:\n",
        "                if collapse_repeated and j != 0 and index == args[j - 1]:\n",
        "                    continue\n",
        "                decode.append(index.item())\n",
        "        decodes.append(text_transform.int_to_text(decode))\n",
        "    return decodes, targets"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9OqoVLnrJsCV"
      },
      "source": [
        "class TransformerModel(torch.nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        input_size=80,\n",
        "        output_size=29,\n",
        "        conv2d_filters=32,\n",
        "        attention_dim=360,\n",
        "        attention_heads=8,\n",
        "        feedforward_dim=1024,\n",
        "        num_layers=10,\n",
        "        dropout=0.1,\n",
        "    ):\n",
        "        super(TransformerModel, self).__init__()\n",
        "\n",
        "        self.conv_in = torch.nn.Sequential(\n",
        "            torch.nn.Conv2d(\n",
        "                1, conv2d_filters, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1)\n",
        "            ),\n",
        "            torch.nn.ReLU(),\n",
        "            torch.nn.Conv2d(\n",
        "                conv2d_filters,\n",
        "                conv2d_filters,\n",
        "                kernel_size=(3, 3),\n",
        "                stride=(2, 2),\n",
        "                padding=(1, 1),\n",
        "            ),\n",
        "            torch.nn.ReLU(),\n",
        "        )\n",
        "        self.conv_out = torch.nn.Sequential(\n",
        "            torch.nn.Linear(conv2d_filters * ((input_size // 2) // 2), attention_dim),\n",
        "            PositionalEncoding(attention_dim, 0.1),\n",
        "        )\n",
        "        positionwise_layer = PositionwiseFeedForward\n",
        "        positionwise_layer_args = (attention_dim, feedforward_dim, dropout)\n",
        "        self.encoder_layer = repeat(\n",
        "            num_layers,\n",
        "            lambda lnum: EncoderLayer(\n",
        "                attention_dim,\n",
        "                MultiHeadedAttention(attention_heads, attention_dim, dropout),\n",
        "                positionwise_layer(*positionwise_layer_args),\n",
        "                dropout,\n",
        "                normalize_before=True,\n",
        "                concat_after=False,\n",
        "            ),\n",
        "        )\n",
        "        self.after_norm = LayerNorm(attention_dim)\n",
        "        self.final_layer = torch.nn.Linear(attention_dim, output_size)\n",
        "\n",
        "    def forward(self, x, ilens):\n",
        "        x = x.unsqueeze(1)  # (b, c, t, f)\n",
        "        x = self.conv_in(x)\n",
        "        b, c, t, f = x.size()\n",
        "        x = self.conv_out(x.transpose(1, 2).contiguous().view(b, t, c * f))\n",
        "        masks = (~make_pad_mask(ilens)[:, None, :])[:, :, ::4].to(x.device)\n",
        "        x, _ = self.encoder_layer(x, masks)\n",
        "        x = self.after_norm(x)\n",
        "        x = self.final_layer(x)\n",
        "        return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d2p_8IjeKkqq"
      },
      "source": [
        "def train(model, device, train_loader, criterion, optimizer, scheduler, epoch):\n",
        "    model.train()\n",
        "    data_len = len(train_loader.dataset)\n",
        "\n",
        "    for batch_idx, _data in enumerate(train_loader):\n",
        "        spectrograms, labels, input_lengths, label_lengths = _data\n",
        "        spectrograms, labels = spectrograms[:, :, :, : max(input_lengths)].to(\n",
        "            device\n",
        "        ), labels.to(\n",
        "            device\n",
        "        )  # (batch, 1, feat_dim, time)\n",
        "        spectrograms = spectrograms.squeeze(1).transpose(\n",
        "            1, 2\n",
        "        )  # (batch, time, feat_dim,)\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        output = model(spectrograms, input_lengths)  # (batch, time, n_classes)\n",
        "        output = F.log_softmax(output, dim=2)\n",
        "        output = output.transpose(0, 1)  # (time, batch, n_class)\n",
        "        input_lengths = [x // 4 for x in input_lengths]\n",
        "\n",
        "        loss = criterion(output, labels, input_lengths, label_lengths)\n",
        "        loss.backward()\n",
        "\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), 5.0)\n",
        "        optimizer.step()\n",
        "        scheduler.step()\n",
        "        if batch_idx % 100 == 0 or batch_idx == data_len:\n",
        "            print(\n",
        "                \"Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}\\tLR: {:.6f}\".format(\n",
        "                    epoch,\n",
        "                    batch_idx * len(spectrograms),\n",
        "                    data_len,\n",
        "                    100.0 * batch_idx / len(train_loader),\n",
        "                    loss.item(),\n",
        "                    scheduler.get_last_lr()[0],\n",
        "                )\n",
        "            )\n",
        "\n",
        "\n",
        "def test(model, device, test_loader, criterion, epoch, blank_label):\n",
        "    print(\"\\nevaluating...\")\n",
        "    model.eval()\n",
        "    test_loss = 0\n",
        "    test_cer, test_wer = [], []\n",
        "    with torch.no_grad():\n",
        "        for i, _data in enumerate(test_loader):\n",
        "            spectrograms, labels, input_lengths, label_lengths = _data\n",
        "            spectrograms, labels = spectrograms.to(device), labels.to(device)\n",
        "            spectrograms = spectrograms.squeeze(1).transpose(\n",
        "                1, 2\n",
        "            )  # (batch time, feat_dim,)\n",
        "\n",
        "            output = model(spectrograms, input_lengths)  # (batch, time, n_class)\n",
        "            output = F.log_softmax(output, dim=2)\n",
        "            output = output.transpose(0, 1)  # (time, batch, n_class)\n",
        "            input_lengths = [x // 4 for x in input_lengths]\n",
        "\n",
        "            loss = criterion(output, labels, input_lengths, label_lengths)\n",
        "            test_loss += loss.item() / len(test_loader)\n",
        "\n",
        "            decoded_preds, decoded_targets = GreedyDecoder(\n",
        "                output.transpose(0, 1), labels, label_lengths, blank_label=blank_label,\n",
        "            )\n",
        "            for j in range(len(decoded_preds)):\n",
        "                test_cer.append(cer(decoded_targets[j], decoded_preds[j]))\n",
        "                test_wer.append(wer(decoded_targets[j], decoded_preds[j]))\n",
        "\n",
        "    avg_cer = sum(test_cer) / len(test_cer)\n",
        "    avg_wer = sum(test_wer) / len(test_wer)\n",
        "\n",
        "    print(\n",
        "        \"Test set: Average loss: {:.4f}, Average CER: {:4f} Average WER: {:.4f}\\n\".format(\n",
        "            test_loss, avg_cer, avg_wer\n",
        "        )\n",
        "    )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MzEbtsB1LKsh"
      },
      "source": [
        "def main(output_size, learning_rate=1e-5, batch_size=10, test_batch_size=7, epochs=10,\n",
        "        train_url=\"train-clean-100\", test_url=\"test-clean\", attention_dim=360, num_layers=10):\n",
        "    \n",
        "    hparams = {\n",
        "        \"input_size\": 80,\n",
        "        \"output_size\": output_size,\n",
        "        \"conv2d_filters\": 32,\n",
        "        \"attention_dim\": attention_dim,\n",
        "        \"attention_heads\": 8,\n",
        "        \"feedforward_dim\": 1024,\n",
        "        \"num_layers\": num_layers,\n",
        "        \"dropout\": 0.1,\n",
        "        \"learning_rate\": learning_rate,\n",
        "        \"batch_size\": batch_size,\n",
        "        \"epochs\": epochs\n",
        "    }\n",
        "\n",
        "    use_cuda = torch.cuda.is_available()\n",
        "    torch.manual_seed(7)\n",
        "    device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
        "\n",
        "    if not os.path.isdir(\"./data\"):\n",
        "        os.makedirs(\"./data\")\n",
        "\n",
        "    train_dataset = torchaudio.datasets.LIBRISPEECH(\"./data\", url=train_url, download=True)\n",
        "    test_dataset = torchaudio.datasets.LIBRISPEECH(\"./data\", url=test_url, download=True)\n",
        "\n",
        "    kwargs = {'num_workers': 1, 'pin_memory': True} if use_cuda else {}\n",
        "    train_loader = data.DataLoader(dataset=train_dataset,\n",
        "                                   batch_size=hparams['batch_size'],\n",
        "                                   shuffle=True,\n",
        "                                   collate_fn=lambda x: data_processing(x, 'train'),\n",
        "                                   **kwargs)\n",
        "    test_loader = data.DataLoader(dataset=test_dataset,\n",
        "                                  batch_size=test_batch_size,\n",
        "                                  shuffle=False,\n",
        "                                  collate_fn=lambda x: data_processing(x, 'valid'),\n",
        "                                  **kwargs)\n",
        "\n",
        "    model = TransformerModel(\n",
        "        hparams['input_size'],\n",
        "        hparams['output_size'],\n",
        "        hparams['conv2d_filters'],\n",
        "        hparams['attention_dim'],\n",
        "        hparams['attention_heads'],\n",
        "        hparams['feedforward_dim'],\n",
        "        hparams['num_layers'],\n",
        "        hparams['dropout']\n",
        "    ).to(device)\n",
        "\n",
        "    print(model)\n",
        "    print('Num Model Parameters', sum([param.nelement() for param in model.parameters()]))\n",
        "\n",
        "    optimizer = optim.AdamW(model.parameters(), hparams['learning_rate'])\n",
        "    criterion = torch.nn.CTCLoss(blank=hparams['output_size'] - 1, zero_infinity=False).to(device)\n",
        "    scheduler = optim.lr_scheduler.OneCycleLR(optimizer, max_lr=hparams['learning_rate'],\n",
        "                                              steps_per_epoch=int(len(train_loader)),\n",
        "                                              epochs=hparams['epochs'],\n",
        "                                              anneal_strategy='linear')\n",
        "\n",
        "    for epoch in range(1, epochs + 1):\n",
        "        !date\n",
        "        train(model, device, train_loader, criterion, optimizer, scheduler, epoch)\n",
        "        test(model, device, test_loader, criterion, epoch, blank_label=hparams['output_size'] - 1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6zTETR-mO50j"
      },
      "source": [
        "# main(output_size=29, learning_rate=1e-3, batch_size=10, test_batch_size=7, epochs=10,\n",
        "#      train_url=\"train-clean-100\", test_url=\"test-clean\", attention_dim=360, num_layers=10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hwKY5D4AO6RE"
      },
      "source": [
        "В силу того, что обучение занимает много времени и ресурсов, здесь и далее приводится выдержка из журнала обучения:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cTt8e_D_O-kD"
      },
      "source": [
        "```python\n",
        "main(output_size=29, learning_rate=1e-3, batch_size=10, test_batch_size=7, epochs=10,\n",
        "     train_url=\"train-clean-100\", test_url=\"test-clean\", attention_dim=360, num_layers=10)\n",
        "```\n",
        "\n",
        "```\n",
        "Num Model Parameters 12_850_957\n",
        "\n",
        "Epoch  1: Average loss: 2.1915, Average CER: 0.563139 Average WER: 1.1611\n",
        "Epoch  2: Average loss: 1.9001, Average CER: 0.487321 Average WER: 1.0551\n",
        "Epoch  3: Average loss: 1.5279, Average CER: 0.384309 Average WER: 0.9229\n",
        "Epoch  4: Average loss: 1.2169, Average CER: 0.309339 Average WER: 0.8344\n",
        "Epoch  5: Average loss: 1.0170, Average CER: 0.261957 Average WER: 0.7316\n",
        "Epoch  6: Average loss: 0.8912, Average CER: 0.232378 Average WER: 0.6713\n",
        "Epoch  7: Average loss: 0.8002, Average CER: 0.212101 Average WER: 0.6262\n",
        "Epoch  8: Average loss: 0.7317, Average CER: 0.192456 Average WER: 0.5777\n",
        "Epoch  9: Average loss: 0.6811, Average CER: 0.181670 Average WER: 0.5540\n",
        "Epoch 10: Average loss: 0.6559, Average CER: 0.173048 Average WER: 0.5307\n",
        "\n",
        "Each epoch takes ~11 minutes to run.\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9BkePhAdPAtt"
      },
      "source": [
        "### <b>Задание №1</b> (5 баллов):\n",
        "На данный момент практически все E2E SOTA решения использую [сабворды](https://dyakonov.org/2019/11/29/%D1%82%D0%BE%D0%BA%D0%B5%D0%BD%D0%B8%D0%B7%D0%B0%D1%86%D0%B8%D1%8F-%D0%BD%D0%B0-%D0%BF%D0%BE%D0%B4%D1%81%D0%BB%D0%BE%D0%B2%D0%B0-subword-tokenization/) (subwords/wordpieces) в качестве таргетов нейронки для распознавания. Нам бы тоже не мешало перейти от графем к сабвордам. Теперь вместо букв (графем) будем распознавать кусочки слов. В качестве такого токенайзера предлагается использовать [Sentencepiece](https://github.com/google/sentencepiece). Главное правильно обернуть его в наш класс TextTransform. Текстовый файл (train_clean_100_text_clean.txt) для обучения токенайзера уже подготовлен и лежит в корневой папке проекта. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Et2FwnJFPCS6"
      },
      "source": [
        "class TextTransformBPE:\n",
        "    def __init__(self, train_text, vocab_size):\n",
        "        \"\"\"Обучение BPE модели на 4000 юнитов.\"\"\"\n",
        "        # Обучение из файла:\n",
        "        spm.SentencePieceTrainer.train(\n",
        "            input=train_text, model_prefix='m', vocab_size=vocab_size,\n",
        "            normalization_rule_name='nfkc_cf', model_type='bpe',\n",
        "        )\n",
        "        # Загружаем обученную модель:\n",
        "        self.model = spm.SentencePieceProcessor(model_file='m.model')\n",
        "\n",
        "    def text_to_int(self, text):\n",
        "        \"\"\"\n",
        "        Преобразование входного текста в последовательность сабвордов в формате их индекса \n",
        "        в BPE модели.\n",
        "        \"\"\"\n",
        "        int_sequence = self.model.encode(text)\n",
        "        return int_sequence\n",
        "\n",
        "    def int_to_text(self, labels):\n",
        "        \"\"\"Преобразование последовательности индексов сабвордов в текст.\"\"\"\n",
        "        labels = list(map(int, labels))\n",
        "        string = self.model.decode(labels)\n",
        "        return string"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OBVrR4P-PEQf"
      },
      "source": [
        "# text_transform = TextTransformBPE(train_text='train_clean_100_text_clean.txt', vocab_size=4000)\n",
        "\n",
        "# main(output_size=4001, learning_rate=1e-3, batch_size=10, test_batch_size=7, epochs=10,\n",
        "#      train_url=\"train-clean-100\", test_url=\"test-clean\", attention_dim=360, num_layers=10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_lQBExMSPF06"
      },
      "source": [
        "```python\n",
        "text_transform = TextTransformBPE(train_text='train_clean_100_text_clean.txt', vocab_size=4000)\n",
        "\n",
        "main(output_size=4001, learning_rate=1e-3, batch_size=10, test_batch_size=7, epochs=10,\n",
        "     train_url=\"train-clean-100\", test_url=\"test-clean\", attention_dim=360, num_layers=10)\n",
        "```\n",
        "\n",
        "```\n",
        "Num Model Parameters 14_284_849\n",
        "\n",
        "Epoch  1: Average loss: 6.6286, Average CER: 0.991988 Average WER: 0.9926\n",
        "Epoch  2: Average loss: 4.8681, Average CER: 0.745415 Average WER: 0.8591\n",
        "Epoch  3: Average loss: 4.3174, Average CER: 0.651380 Average WER: 0.7959\n",
        "Epoch  4: Average loss: 3.7446, Average CER: 0.567355 Average WER: 0.7276\n",
        "Epoch  5: Average loss: 3.1421, Average CER: 0.465592 Average WER: 0.6465\n",
        "Epoch  6: Average loss: 2.7075, Average CER: 0.398419 Average WER: 0.5818\n",
        "Epoch  7: Average loss: 2.4468, Average CER: 0.359280 Average WER: 0.5419\n",
        "Epoch  8: Average loss: 2.2605, Average CER: 0.324294 Average WER: 0.5165\n",
        "Epoch  9: Average loss: 2.1211, Average CER: 0.303219 Average WER: 0.4925\n",
        "Epoch 10: Average loss: 2.0560, Average CER: 0.293074 Average WER: 0.4815\n",
        "\n",
        "Each epoch takes ~11 minutes to run.\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3onpva-lPHXy"
      },
      "source": [
        "CER для этой модели выше, чем для модели с обычным `TextTransform`: это связано с тем, что число возможных выходов в разы больше, и модели сложнее выбирать правильный. Но итоговая ошибка WER стала меньше. Ниже мы попробуем добиться ещё более хороших результатов."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rR9ZExZrPICD"
      },
      "source": [
        "### <b>Задание №2</b> (5 баллов):\n",
        "Импровизация по улучшению качества распознавания."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vFi7QtJ-PJ1O"
      },
      "source": [
        "Увеличим размер словаря до 10000, размерность вектора внимания `attention_dim` до 512, число слоёв трансформера до 16, увеличим максимальное значение `learning_rate` до 0.01:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X5WAIlkLPMLs"
      },
      "source": [
        "# text_transform = TextTransformBPE(train_text='train_clean_100_text_clean.txt', vocab_size=10000)\n",
        "\n",
        "# main(output_size=10001, learning_rate=1e-2, batch_size=6, test_batch_size=4, epochs=10,\n",
        "#      train_url=\"train-clean-100\", test_url=\"test-clean\", attention_dim=512, num_layers=16)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4AW0j_uJPMop"
      },
      "source": [
        "```python\n",
        "text_transform = TextTransformBPE(train_text='train_clean_100_text_clean.txt', vocab_size=10000)\n",
        "\n",
        "main(output_size=10001, learning_rate=1e-2, batch_size=6, test_batch_size=4, epochs=10,\n",
        "     train_url=\"train-clean-100\", test_url=\"test-clean\", attention_dim=512, num_layers=16)\n",
        "```\n",
        "\n",
        "```\n",
        "Num Model Parameters 39_113_841\n",
        "\n",
        "Epoch  1: Average loss: 6.2865, Average CER: 0.986621 Average WER: 0.9878\n",
        "Epoch  2: Average loss: 6.1903, Average CER: 0.883747 Average WER: 0.9467\n",
        "Epoch  3: Average loss: 5.9818, Average CER: 0.851486 Average WER: 0.9364\n",
        "Epoch  4: Average loss: 5.8568, Average CER: 0.796728 Average WER: 0.9136\n",
        "Epoch  5: Average loss: 5.8445, Average CER: 0.843930 Average WER: 0.9451\n",
        "Epoch  6: Average loss: 5.5203, Average CER: 0.809913 Average WER: 0.9250\n",
        "Epoch  7: Average loss: 5.3197, Average CER: 0.737294 Average WER: 0.8970\n",
        "Epoch  8: Average loss: 5.0723, Average CER: 0.749833 Average WER: 0.8892\n",
        "Epoch  9: Average loss: 4.8846, Average CER: 0.717338 Average WER: 0.8749\n",
        "Epoch 10: Average loss: 4.7430, Average CER: 0.706446 Average WER: 0.8638\n",
        "\n",
        "Each epoch takes ~15 minutes to run.\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5-BSNaGePOO_"
      },
      "source": [
        "Как можно заметить, результаты ухудшились. Во-первых, выросло количество параметров модели, но число эпох осталось тем же, в связи с чем модель могла не успеть обучиться так же хорошо, как при меньшем количестве параметров. Во-вторых, ещё сильнее выросло число возможных выходов, из-за чего модели стало сложнее делать верные предсказания (по крайней мере, на начальных эпохах)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JOfMkGDXPQnj"
      },
      "source": [
        "Попробуем вернуться к первому варианту `TextTransformBPE`, но уменьшим размер словаря вдвое."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5XHpXfKDPTMY"
      },
      "source": [
        "# text_transform = TextTransformBPE(train_text='train_clean_100_text_clean.txt', vocab_size=2000)\n",
        "\n",
        "# main(output_size=2001, learning_rate=1e-3, batch_size=10, test_batch_size=7, epochs=10,\n",
        "#      train_url=\"train-clean-100\", test_url=\"test-clean\", attention_dim=360, num_layers=10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HhDzj24JPTnv"
      },
      "source": [
        "```python\n",
        "text_transform = TextTransformBPE(train_text='train_clean_100_text_clean.txt', vocab_size=2000)\n",
        "\n",
        "main(output_size=2001, learning_rate=1e-3, batch_size=10, test_batch_size=7, epochs=10,\n",
        "     train_url=\"train-clean-100\", test_url=\"test-clean\", attention_dim=360, num_layers=10)\n",
        "```\n",
        "\n",
        "```\n",
        "Num Model Parameters 13_562_849\n",
        "\n",
        "Epoch  1: Average loss: 5.9217, Average CER: 0.933404 Average WER: 0.9800\n",
        "Epoch  2: Average loss: 4.4754, Average CER: 0.761548 Average WER: 0.8589\n",
        "Epoch  3: Average loss: 4.0477, Average CER: 0.657061 Average WER: 0.8101\n",
        "Epoch  4: Average loss: 3.2298, Average CER: 0.486938 Average WER: 0.6984\n",
        "Epoch  5: Average loss: 2.7068, Average CER: 0.400269 Average WER: 0.6330\n",
        "Epoch  6: Average loss: 2.4311, Average CER: 0.358651 Average WER: 0.6001\n",
        "Epoch  7: Average loss: 2.1797, Average CER: 0.329015 Average WER: 0.5538\n",
        "Epoch  8: Average loss: 1.9977, Average CER: 0.296656 Average WER: 0.5237\n",
        "Epoch  9: Average loss: 1.8694, Average CER: 0.275591 Average WER: 0.5008\n",
        "Epoch 10: Average loss: 1.8079, Average CER: 0.266416 Average WER: 0.4870\n",
        "\n",
        "Each epoch takes ~11 minutes to run.\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C4ly7iR-PVSS"
      },
      "source": [
        "При уменьшении размера словаря модели стало проще правильно предсказывать отдельные символы (CER уменьшилась). Ошибка WER осталась практически без изменений."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gesyL24YPWx8"
      },
      "source": [
        "Посмотрим, что произвдёт с качеством распознавания при дальшейшем уменьшении размера словаря."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H4qZNo_cPYfR"
      },
      "source": [
        "# text_transform = TextTransformBPE(train_text='train_clean_100_text_clean.txt', vocab_size=1000)\n",
        "\n",
        "# main(output_size=1001, learning_rate=1e-3, batch_size=10, test_batch_size=7, epochs=10,\n",
        "#      train_url=\"train-clean-100\", test_url=\"test-clean\", attention_dim=360, num_layers=10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZFrRNMmWPady"
      },
      "source": [
        "```python\n",
        "text_transform = TextTransformBPE(train_text='train_clean_100_text_clean.txt', vocab_size=1000)\n",
        "\n",
        "main(output_size=1001, learning_rate=1e-3, batch_size=10, test_batch_size=7, epochs=10,\n",
        "     train_url=\"train-clean-100\", test_url=\"test-clean\", attention_dim=360, num_layers=10)\n",
        "```\n",
        "\n",
        "```\n",
        "Num Model Parameters 13_201_849\n",
        "\n",
        "Epoch  1: Average loss: 5.0266, Average CER: 0.823378 Average WER: 0.9272\n",
        "Epoch  2: Average loss: 4.0076, Average CER: 0.679294 Average WER: 0.8462\n",
        "Epoch  3: Average loss: 3.6146, Average CER: 0.614216 Average WER: 0.8040\n",
        "Epoch  4: Average loss: 2.9051, Average CER: 0.446937 Average WER: 0.7264\n",
        "Epoch  5: Average loss: 2.4120, Average CER: 0.381286 Average WER: 0.6491\n",
        "Epoch  6: Average loss: 2.0948, Average CER: 0.332462 Average WER: 0.6040\n",
        "Epoch  7: Average loss: 1.8891, Average CER: 0.301625 Average WER: 0.5645\n",
        "Epoch  8: Average loss: 1.7378, Average CER: 0.277611 Average WER: 0.5341\n",
        "Epoch  9: Average loss: 1.6393, Average CER: 0.258625 Average WER: 0.5125\n",
        "Epoch 10: Average loss: 1.5818, Average CER: 0.247516 Average WER: 0.4969\n",
        "\n",
        "Each epoch takes ~11 minutes to run.\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v2JEcNehPb7q"
      },
      "source": [
        "Мы пробовали увеличивать словарь только одновременно с увеличением размерностей внутренних слоёв модели. Теперь попробуем увеличить размер словаря при тех же остальных параметрах. Ожидается увеличение CER и как минимум не ухудшение WER."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m6qCoFJpPdoJ"
      },
      "source": [
        "# text_transform = TextTransformBPE(train_text='train_clean_100_text_clean.txt', vocab_size=6000)\n",
        "\n",
        "# main(output_size=6001, learning_rate=1e-3, batch_size=10, test_batch_size=7, epochs=10,\n",
        "#      train_url=\"train-clean-100\", test_url=\"test-clean\", attention_dim=360, num_layers=10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QPplPrbpPfSf"
      },
      "source": [
        "```python\n",
        "text_transform = TextTransformBPE(train_text='train_clean_100_text_clean.txt', vocab_size=6000)\n",
        "\n",
        "main(output_size=6001, learning_rate=1e-3, batch_size=10, test_batch_size=7, epochs=10,\n",
        "     train_url=\"train-clean-100\", test_url=\"test-clean\", attention_dim=360, num_layers=10)\n",
        "```\n",
        "\n",
        "```\n",
        "Num Model Parameters 15_006_849\n",
        "\n",
        "Epoch  1: Average loss: 6.8542, Average CER: 0.995195 Average WER: 0.9975\n",
        "Epoch  2: Average loss: 5.3233, Average CER: 0.877091 Average WER: 0.9276\n",
        "Epoch  3: Average loss: 4.7425, Average CER: 0.783739 Average WER: 0.8492\n",
        "Epoch  4: Average loss: 4.3134, Average CER: 0.742241 Average WER: 0.8131\n",
        "Epoch  5: Average loss: 3.9123, Average CER: 0.665480 Average WER: 0.7532\n",
        "Epoch  6: Average loss: 3.4705, Average CER: 0.586426 Average WER: 0.6887\n",
        "Epoch  7: Average loss: 3.0790, Average CER: 0.517929 Average WER: 0.6345\n",
        "Epoch  8: Average loss: 2.8149, Average CER: 0.450333 Average WER: 0.5870\n",
        "Epoch  9: Average loss: 2.6452, Average CER: 0.422564 Average WER: 0.5628\n",
        "Epoch 10: Average loss: 2.5583, Average CER: 0.406916 Average WER: 0.5453\n",
        "\n",
        "Each epoch takes ~11 minutes to run.\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I4sOWzBOPgr_"
      },
      "source": [
        "Ошибка CER действительно увеличилась, но, кроме того, немного выросла и WER."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DQnFa-m_PkWh"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}